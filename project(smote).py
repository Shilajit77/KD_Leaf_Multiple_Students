# -*- coding: utf-8 -*-
"""Project(Smote).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XlEQ0eFhigvSM7DFKSOXG_bWc9Zs1ZWU

# Data Loading and Preprocessing
"""

#! pip install kaggle
#! mkdir ~/.kaggle
! cp kaggle.json ~/.kaggle/
! chmod 600 ~/.kaggle/kaggle.json
!kaggle datasets download -d csafrit2/plant-leaves-for-image-classification

!unzip /content/plant-leaves-for-image-classification.zip

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
import numpy as np
import cv2
import os
import PIL
import tensorflow as tf
import torch
import torch
import torchvision
import numpy as np
import matplotlib
import matplotlib.pyplot as plt
import torch.nn as nn
import torch.nn.functional as F
from torchvision.datasets import MNIST
from torchvision.transforms import ToTensor
from torchvision.utils import make_grid
from torch.utils.data.dataloader import DataLoader
from torch.utils.data import random_split
# %matplotlib inline
import os
import torch
import torchvision
import tarfile
from torchvision.datasets.utils import download_url
from torch.utils.data import random_split
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np 
import warnings
warnings.filterwarnings("ignore")
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
device

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import torch
from torchvision import transforms
from torchvision.datasets.folder import ImageFolder
import matplotlib.pyplot as plt
import numpy as np
import cv2
import os
import PIL
import time
from tqdm import tqdm
from timeit import default_timer as timer
import tensorflow as tf
import torch
import warnings
import torch
import torchvision
import numpy as np
import matplotlib
import matplotlib.pyplot as plt
import torch.nn as nn
import torch.nn.functional as F
from torchvision.datasets import CIFAR10
from torchvision.transforms import ToTensor
from tqdm import tqdm
from torchvision.utils import make_grid
from torch.utils.data.dataloader import DataLoader
from torch.utils.data import random_split
# %matplotlib inline
import os
import torch
import torchvision
import tarfile
from torchvision.datasets.utils import download_url
from torch.utils.data import random_split
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np 
import torchvision.transforms as transforms
import torch.nn as nn
import torch.nn.functional as F
warnings.filterwarnings("ignore")
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
device

import pathlib
data_dir = '/content/Plants_2/train//'
data_dir = pathlib.Path(data_dir)
path = data_dir
path

folder_list = ['Alstonia Scholaris diseased (P2a)','Alstonia Scholaris healthy (P2b)','Arjun diseased (P1a)',
              'Arjun healthy (P1b)','Bael diseased (P4b)','Basil healthy (P8)','Chinar diseased (P11b)','Chinar healthy (P11a)',
              'Gauva diseased (P3b)','Gauva healthy (P3a)','Jamun diseased (P5b)','Jamun healthy (P5a)','Jatropha diseased (P6b)',
              'Jatropha healthy (P6a)','Lemon diseased (P10b)','Lemon healthy (P10a)','Mango diseased (P0b)','Mango healthy (P0a)',
              'Pomegranate diseased (P9b)','Pomegranate healthy (P9a)','Pongamia Pinnata diseased (P7b)','Pongamia Pinnata healthy (P7a)']

x = []
y = []
#data_dir.glob('Alstonia Scholaris diseased (P2a)/*')
i = 1
for folder in folder_list:
    string = folder + '/*'
    images = list(data_dir.glob(string))
    for img in images:
        pic = cv2.resize(cv2.imread(str(img)),(64,64))
        x.append(pic) 
        y.append(folder)
    print(f'Folder {i} complete.\n')
    i = i+1

from sklearn import preprocessing
le = preprocessing.LabelEncoder()
y = le.fit_transform(y)
x = np.array(x,dtype='float32')
x = x/255
plt.imshow(x[0])

height = 64
width = 64

from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x, y,test_size=0.30)
x_val, x_test, y_val, y_test = train_test_split(x_test, y_test,test_size=0.66)
print(x_train.shape,x_val.shape,x_test.shape)

pd.DataFrame(le.inverse_transform(y_train)).value_counts()

"""# Oversampling (Smote)"""

from imblearn.over_sampling import SMOTE
x_train = x_train.reshape(2991, 64*64*3)
smote = SMOTE(sampling_strategy = 'all')
x_smote, y_smote = smote.fit_resample(x_train , y_train)
x_smote.shape
#X_smote = x_smote.reshape(10800, 28, 28, 3)

x_smote = x_smote.reshape(5192, 64, 64, 3)

pd.DataFrame(le.inverse_transform(y_smote)).value_counts()

a = torch.tensor(x_smote)
b = torch.tensor(y_smote)
c = torch.tensor(x_test)
d = torch.tensor(y_test)
e = torch.tensor(x_val)
f = torch.tensor(y_val)
d = d.type(torch.LongTensor)
b = b.type(torch.LongTensor)
f = f.type(torch.LongTensor)
a =a.reshape(5192,3,height,width)
c = c.reshape(847,3,height,width)
e = e.reshape(436,3,height,width)
print(a.shape)
print(c.shape)
print(e.shape)
from torch.utils.data import TensorDataset,DataLoader
tensor_data1 = TensorDataset(a,b)
tensor_data2 = TensorDataset(c,d)
tensor_data3 = TensorDataset(e,f)
teacher_loader = DataLoader(tensor_data1,shuffle=1,batch_size=128)
test_loader = DataLoader(tensor_data2,shuffle=1,batch_size=128)
val_loader = DataLoader(tensor_data3,shuffle=1,batch_size=128)

torch.save(teacher_loader,'tensor_data1.pt')
torch.save(test_loader,'tesnsor_data2.pt')
torch.save(val_loader,'tesnsor_data3.pt')

"""# Teacher Traning"""

tmodel = torchvision.models.densenet121()
tmodel.classifier = nn.Linear(in_features=1024, out_features=22, bias=True)
for name, child in tmodel.named_children():
   if name in ['features','avgpool','classifier']:
       print(name + ' is unfrozen')
       for param in child.parameters():
           param.requires_grad = True
   else:
       print(name + ' is frozen')
       for param in child.parameters():
           param.requires_grad = False
optimizer2 = torch.optim.Adadelta(filter(lambda p: p.requires_grad, tmodel.parameters()), lr=0.05)
#optimizer2 = torch.optim.SGD(model.parameters(), lr=0.5)
import time
from timeit import default_timer as timer
tmodel = tmodel.cuda()

train_loss = []
val_los = []
train_acc = []
val_acc = []
for i in range(10):
        l1 = []
        l2 = []
        a1 = []
        a2 = []
        start = timer()
        for img,labels in tqdm(teacher_loader, desc='Training', unit=' unit',leave=False,colour='red'):
            



           # print(end - start)
            
            
            
            img = img.to(device)
            labels = labels.to(device)
            output = tmodel(img)
            probs = F.softmax(output,dim=1)
            max_prob,preds = torch.max(probs,dim=1)
            acc1 = (torch.sum(preds == labels).item() / len(preds))
            
            
            #accu=[]
            a1.append(acc1)
            #gg = np.mean(accu)
            
            loss = nn.CrossEntropyLoss()(output,labels)
            l1.append(loss.item())
            loss.backward()
            optimizer2.step()
            optimizer2.zero_grad()
            
        for a,b in tqdm(val_loader, desc='Training', unit=' unit',leave=False,colour='red'):
                a = a.to(device)
                b = b.to(device)
                rr = tmodel(a)
                val_loss = nn.CrossEntropyLoss()(rr,b)
                l2.append(val_loss.item())
                probs = F.softmax(rr,dim=1)
                max_prob,preds = torch.max(probs,dim=1)
                acc = (torch.sum(preds == b).item() / len(preds))
                a2.append(acc)
        train_loss.append(np.mean(l1))
        val_los.append(np.mean(l2))
        train_acc.append(np.mean(a1))
        val_acc.append(np.mean(a2))
        end = timer()
        time = end - start
        time = time/60
        print(f'Epoch {i+1}/10: Train Loss==> {np.mean(l1):.2f} Val Loss==> {np.mean(l2):.2f} Train_Acc==> {np.mean(a1):.2f} Val_Acc==> {np.mean(a2):.2f}')   
        #print(i+1,loss,acc)

fig = plt.figure(figsize=(12, 5))
fig.add_subplot(1, 2, 1)
  
# showing image
plt.plot(train_loss,label='Training')
plt.plot(val_los,label='Testing')
plt.legend()
plt.title("Loss of Teacher Model")
fig.add_subplot(1, 2, 2)
  
# showing image
plt.plot(train_acc,label='Training')
plt.plot(val_acc,label='Testing')
plt.legend()
plt.title("Accuracy of Teacher Model")
plt.show()

import joblib
joblib.dump(tmodel, 'tmodel1.pkl')

start = timer()
predictions = []
accu = []
l_act = []
i = 0
for images_test,labels_test in test_loader:
    #i = i+1```````````````````````````````
    images_test = images_test.to(device)
    labels_test = labels_test.to(device)
    ypred = tmodel(images_test)
    probs = F.softmax(ypred,dim=1)
    max_prob,preds = torch.max(probs,dim=1)
    #print(preds)
    #print(max_prob)
    acc = (torch.sum(preds == labels_test).item() / len(preds))
    accu.append(acc)
    predictions.append(preds)
    l_act.append(labels_test)
    
print(f'Accuracy of Teacher(Densenet121) is : {np.mean(accu):.2f}')
predictions = [t.cpu().numpy() for t in predictions]
predictions = np.array(np.concatenate(predictions))
pd.DataFrame(predictions).value_counts()
ytrue = [t.cpu().numpy() for t in l_act]
ytrue = np.array(np.concatenate(ytrue))
pd.DataFrame(ytrue).value_counts()
report = pd.DataFrame()
report['Actual'] = ytrue
report['Predicted'] = predictions
report['Actual1'] = le.inverse_transform(report['Actual'])
report['Predicted1'] = le.inverse_transform(report['Predicted'])
report5 = report.copy()
from sklearn.metrics import classification_report
print(classification_report(report['Actual1'], report['Predicted1']))

tmodel2 = torchvision.models.resnet101()
tmodel2.fc = nn.Linear(in_features=2048, out_features=22, bias=True)
for name, child in tmodel2.named_children():
   if name in ['conv1','layer1','layer2','layer3','layer4','fc']:
       print(name + ' is unfrozen')
       for param in child.parameters():
           param.requires_grad = True
   else:
       print(name + ' is frozen')
       for param in child.parameters():
           param.requires_grad = False
optimizer2 = torch.optim.Adadelta(filter(lambda p: p.requires_grad, tmodel2.parameters()), lr=0.4,weight_decay=0.0004)
#optimizer2 = torch.optim.SGD(model.parameters(), lr=0.5)
import time
from timeit import default_timer as timer
tmodel2 = tmodel2.cuda()

train_loss = []
val_los = []
train_acc = []
val_acc = []
for i in range(10):
        l1 = []
        l2 = []
        a1 = []
        a2 = []
        start = timer()
        for img,labels in tqdm(teacher_loader, desc='Training', unit=' unit',leave=False,colour='red'):
            



           # print(end - start)
            
            
            
            img = img.to(device)
            labels = labels.to(device)
            output = tmodel2(img)
            probs = F.softmax(output,dim=1)
            max_prob,preds = torch.max(probs,dim=1)
            acc1 = (torch.sum(preds == labels).item() / len(preds))
            
            
            #accu=[]
            a1.append(acc1)
            #gg = np.mean(accu)
            
            loss = nn.CrossEntropyLoss()(output,labels)
            l1.append(loss.item())
            loss.backward()
            optimizer2.step()
            optimizer2.zero_grad()
            
        for a,b in tqdm(val_loader, desc='Training', unit=' unit',leave=False,colour='red'):
                a = a.to(device)
                b = b.to(device)
                rr = tmodel2(a)
                val_loss = nn.CrossEntropyLoss()(rr,b)
                l2.append(val_loss.item())
                probs = F.softmax(rr,dim=1)
                max_prob,preds = torch.max(probs,dim=1)
                acc = (torch.sum(preds == b).item() / len(preds))
                a2.append(acc)
        train_loss.append(np.mean(l1))
        val_los.append(np.mean(l2))
        train_acc.append(np.mean(a1))
        val_acc.append(np.mean(a2))
        end = timer()
        time = end - start
        time = time/60
        print(f'Epoch {i+1}/15: Train Loss==> {np.mean(l1):.2f} Val Loss==> {np.mean(l2):.2f} Train_Acc==> {np.mean(a1):.2f} Val_Acc==> {np.mean(a2):.2f}')   
        #print(i+1,loss,acc)

import joblib
joblib.dump(tmodel2, 'tmodel2.pkl')

fig = plt.figure(figsize=(12, 5))
fig.add_subplot(1, 2, 1)
  
# showing image
plt.plot(train_loss,label='Training')
plt.plot(val_los,label='Testing')
plt.legend()
plt.title("Loss of Teacher Model(ResNet101)")
fig.add_subplot(1, 2, 2)
  
# showing image
plt.plot(train_acc,label='Training')
plt.plot(val_acc,label='Testing')
plt.legend()
plt.title("Accuracy of Teacher Model(Resnet101)")
plt.show()

start = timer()
predictions = []
accu = []
l_act = []
i = 0
for images_test,labels_test in test_loader:
    #i = i+1```````````````````````````````
    images_test = images_test.to(device)
    labels_test = labels_test.to(device)
    ypred = tmodel2(images_test)
    probs = F.softmax(ypred,dim=1)
    max_prob,preds = torch.max(probs,dim=1)
    #print(preds)
    #print(max_prob)
    acc = (torch.sum(preds == labels_test).item() / len(preds))
    accu.append(acc)
    predictions.append(preds)
    l_act.append(labels_test)
    
print(f'Accuracy of Teacher(ResNet101) is : {np.mean(accu):.2f}')
predictions = [t.cpu().numpy() for t in predictions]
predictions = np.array(np.concatenate(predictions))
pd.DataFrame(predictions).value_counts()
ytrue = [t.cpu().numpy() for t in l_act]
ytrue = np.array(np.concatenate(ytrue))
pd.DataFrame(ytrue).value_counts()
report = pd.DataFrame()
report['Actual'] = ytrue
report['Predicted'] = predictions
report['Actual1'] = le.inverse_transform(report['Actual'])
report['Predicted1'] = le.inverse_transform(report['Predicted'])
report5 = report.copy()
from sklearn.metrics import classification_report
print(classification_report(report['Actual1'], report['Predicted1']))

"""# Student"""

sdata1,sdata2 = random_split(tensor_data1,[5000,192])
sdata1,sdata2 = random_split(sdata1,[2500,2500])

sloader1 = DataLoader((sdata1),shuffle=1,batch_size=64)
sloader2 = DataLoader((sdata2),shuffle=1,batch_size=64)

import torch
import torch.nn as nn

class smodel(nn.Module):
    def __init__(self):
        super(smodel, self).__init__()
        
        # Define convolutional layers
        self.conv1 = nn.Conv2d(in_channels=3, out_channels=8, kernel_size=3, padding=1)
        self.relu1 = nn.ReLU()
        self.pool1 = nn.MaxPool2d(kernel_size=5, stride=2)
        self.conv2 = nn.Conv2d(in_channels=8, out_channels=8, kernel_size=3, padding=1)
        self.relu2 = nn.ReLU()
        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)
        self.conv3 = nn.Conv2d(in_channels=8, out_channels=8, kernel_size=3, padding=1)
        self.relu3 = nn.ReLU()
        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)
        self.conv4 = nn.Conv2d(in_channels=8, out_channels=8, kernel_size=3, padding=1)
        self.relu4 = nn.ReLU()
        # Define fully connected layers
        self.fc1 = nn.Linear(in_features=392 , out_features=22)
        #self.relu3 = nn.ReLU()
        #self.fc2 = nn.Linear(in_features=250, out_features=22)
        
    def forward(self, x):
        x = self.conv1(x)
        x = self.relu1(x)
        x = self.pool1(x)
        x = self.conv2(x)
        x = self.relu2(x)
        x = self.pool2(x)
        x = self.conv3(x)
        x = self.relu3(x)
        x = self.pool3(x)
        x = self.conv4(x)
        x = self.relu4(x)
        #print(x.shape)
        x = torch.flatten(x,1)
        #print(x.shape)
        x = self.fc1(x)
        #x = self.relu3(x)
        #x = self.fc2(x)
        #print(x.shape)
        return x

smodel3 = smodel()
smodel3 = smodel3.cuda()
smodel4 = smodel()
smodel4 = smodel4.cuda()
optimizers = torch.optim.SGD(smodel4.parameters(), lr=0.03,momentum=0.9)
optimizers2 = torch.optim.Adadelta(smodel3.parameters(), lr=0.3)

train_loss = []
val_los = []
train_acc = []
val_acc = []
for i in range(100):
        l1 = []
        l2 = []
        a1 = []
        a2 = []
        start = timer()
        for img,labels in tqdm(sloader2, desc='Training', unit=' unit',leave=False,colour='red'):
            



           # print(end - start)
            
            
            
            img = img.to(device)
            labels = labels.to(device)
            
            output = smodel3(img)
            #print(output.shape)
            probs = F.softmax(output,dim=1)
            max_prob,preds = torch.max(probs,dim=1)
            #print(len(labels))
            acc1 = (torch.sum(preds == labels).item() / len(preds))
            #compare = smodel3(img)
            #pdist = F.softmax(compare/4,dim=1)  ##Teacher distribution
            #max_prob1,preds1 = torch.max(pdist,dim=1)   #Teacher output
            #accu=[]
            a1.append(acc1)
            #gg = np.mean(accu)
            #soft_loss_teacher = nn.KLDivLoss(reduction='batchmean')(F.log_softmax(output / 4, dim=1), pdist)
            loss = nn.CrossEntropyLoss()(output,labels)
            #loss = 0.2*loss + 0.8*soft_loss_teacher
            l1.append(loss.item())
            loss.backward()
            optimizers2.step()
            optimizers2.zero_grad()
            
        for a,b in tqdm(val_loader, desc='Training', unit=' unit',leave=False,colour='red'):
                a = a.to(device)
                b = b.to(device)
                rr = smodel3(a)
                val_loss = nn.CrossEntropyLoss()(rr,b)
                l2.append(val_loss.item())
                probs = F.softmax(rr,dim=1)
                max_prob,preds = torch.max(probs,dim=1)
                acc = (torch.sum(preds == b).item() / len(preds))
                a2.append(acc)
        train_loss.append(np.mean(l1))
        val_los.append(np.mean(l2))
        train_acc.append(np.mean(a1))
        val_acc.append(np.mean(a2))
        end = timer()
        time = end - start
        time = time/60
        print(f'Epoch {i+1}/100: Train Loss==> {np.mean(l1):.2f} Val Loss==> {np.mean(l2):.2f} Train_Acc==> {np.mean(a1):.2f} Val_Acc==> {np.mean(a2):.2f}')   
        #print(i+1,loss,acc)

fig = plt.figure(figsize=(12, 5))
fig.add_subplot(1, 2, 1)
  
# showing image
plt.plot(train_loss,label='Training')
plt.plot(val_los,label='Testing')
plt.legend()
plt.title("Loss of Student(Coustom model Without KD) Model")
fig.add_subplot(1, 2, 2)
  
# showing image
plt.plot(train_acc,label='Training')
plt.plot(val_acc,label='Testing')
plt.legend()
plt.title("Accuracy of Student Model(Coustom model Without KD)")
plt.show()

classification_report(smodel3,test_loader)



train_loss1 = []
val_los1 = []
train_acc1 = []
val_acc1 = []
for i in range(100):
        l1 = []
        l2 = []
        a1 = []
        a2 = []
        start = timer()
        for img,labels in tqdm(sloader2, desc='Training', unit=' unit',leave=False,colour='red'):
            



           # print(end - start)
            
            
            
            img = img.to(device)
            labels = labels.to(device)
            
            output = smodel4(img)
            #print(output.shape)
            probs = F.softmax(output,dim=1)
            max_prob,preds = torch.max(probs,dim=1)
            #print(len(labels))
            acc1 = (torch.sum(preds == labels).item() / len(preds))
            compare = tmodel(img)
            pdist = F.softmax(compare/4,dim=1)  ##Teacher distribution
            max_prob1,preds1 = torch.max(pdist,dim=1)   #Teacher output
            #accu=[]
            a1.append(acc1)
            #gg = np.mean(accu)
            soft_loss_teacher = nn.KLDivLoss(reduction='batchmean')(F.log_softmax(output /4, dim=1), pdist)
            loss = nn.CrossEntropyLoss()(output,labels)
            loss = 0.4*1*loss + 0.6*1*soft_loss_teacher
            l1.append(loss.item())
            loss.backward()
            optimizers.step()
            optimizers.zero_grad()
            
        for a,b in tqdm(val_loader, desc='Training', unit=' unit',leave=False,colour='red'):
                a = a.to(device)
                b = b.to(device)
                rr = smodel4(a)
                val_loss = nn.CrossEntropyLoss()(rr,b)
                l2.append(val_loss.item())
                probs = F.softmax(rr,dim=1)
                max_prob,preds = torch.max(probs,dim=1)
                acc = (torch.sum(preds == b).item() / len(preds))
                a2.append(acc)
        train_loss1.append(np.mean(l1))
        val_los1.append(np.mean(l2))
        train_acc1.append(np.mean(a1))
        val_acc1.append(np.mean(a2))
        end = timer()
        time = end - start
        time = time/60
        print(f'Epoch {i+1}/100: Train Loss==> {np.mean(l1):.2f} Val Loss==> {np.mean(l2):.2f} Train_Acc==> {np.mean(a1):.2f} Val_Acc==> {np.mean(a2):.2f}')   
        #print(i+1,loss,acc)

fig = plt.figure(figsize=(12, 5))
fig.add_subplot(1, 2, 1)
  
# showing image
plt.plot(train_loss1,label='Training')
plt.plot(val_los1,label='Testing')
plt.legend()
plt.title("Loss of Student(Custom With KD) Model")
fig.add_subplot(1, 2, 2)
  
# showing image
plt.plot(train_acc1,label='Training')
plt.plot(val_acc1,label='Testing')
plt.legend()
plt.title("Accuracy of Student (Custom With KD)")
plt.show()

classification_report(smodel4,test_loader)

plt.plot(val_acc,label='Without KD')
plt.plot(val_acc1,label='With KD')
plt.legend()
plt.title("Accuracy of Student (Custom Model)")
plt.show()

import joblib
joblib.dump(smodel4, 'csmodel_dnet.pkl')

def classification_report(model,test_loader):
  predictions = []
  accu = []
  l_act = []
  i = 0
  for images_test,labels_test in test_loader:
      #i = i+1```````````````````````````````
      images_test = images_test.to(device)
      labels_test = labels_test.to(device)
      ypred = model(images_test)
      probs = F.softmax(ypred,dim=1)
      max_prob,preds = torch.max(probs,dim=1)
      #print(preds)
      #print(max_prob)
      acc = (torch.sum(preds == labels_test).item() / len(preds))
      accu.append(acc)
      predictions.append(preds)
      l_act.append(labels_test)
      
  print(f'Accuracy of model is : {np.mean(accu):.2f}')
  predictions = [t.cpu().numpy() for t in predictions]
  predictions = np.array(np.concatenate(predictions))
  pd.DataFrame(predictions).value_counts()
  ytrue = [t.cpu().numpy() for t in l_act]
  ytrue = np.array(np.concatenate(ytrue))
  pd.DataFrame(ytrue).value_counts()
  report = pd.DataFrame()
  report['Actual'] = ytrue
  report['Predicted'] = predictions
  report['Actual1'] = le.inverse_transform(report['Actual'])
  report['Predicted1'] = le.inverse_transform(report['Predicted'])
  report5 = report.copy()
  from sklearn.metrics import classification_report
  print(classification_report(report['Actual1'], report['Predicted1']))

"""# DenseNET Teacher"""

smodel1 = torch.hub.load('pytorch/vision:v0.10.0', 'shufflenet_v2_x1_0', pretrained=1)
smodel1.fc = nn.Sequential(
    nn.Linear(in_features = 1024,out_features=22),
)

for name, child in smodel1.named_children():
   if name in ['stage3','stage4','conv5','fc']:
       print(name + ' is unfrozen')
       for param in child.parameters():
           param.requires_grad = True
   else:
       print(name + ' is frozen')
       for param in child.parameters():
           param.requires_grad = False

smodel2 = torch.hub.load('pytorch/vision:v0.10.0', 'shufflenet_v2_x1_0', pretrained=1)
smodel2.fc = nn.Sequential(
    nn.Linear(in_features = 1024,out_features=22),
)

for name, child in smodel2.named_children():
   if name in ['stage3','stage4','conv5','fc']:
       print(name + ' is unfrozen')
       for param in child.parameters():
           param.requires_grad = True
   else:
       print(name + ' is frozen')
       for param in child.parameters():
           param.requires_grad = False
optimizers = torch.optim.Adadelta(filter(lambda p: p.requires_grad, smodel1.parameters()), lr=0.02)
#smodel2 = smodel()
smodel1 = smodel1.cuda()
smodel2 = smodel2.cuda()
#optimizers = torch.optim.Adadelta(smodel1.parameters(), lr=0.6,weight_decay=1e-5)
optimizers2 = torch.optim.Adadelta(filter(lambda p: p.requires_grad, smodel2.parameters()), lr=0.02)

train_loss = []
val_los = []
train_acc = []
val_acc = []
for i in range(100):
        l1 = []
        l2 = []
        a1 = []
        a2 = []
        start = timer()
        for img,labels in tqdm(sloader1, desc='Training', unit=' unit',leave=False,colour='red'):
            



           # print(end - start)
            
            
            
            img = img.to(device)
            labels = labels.to(device)
            
            output = smodel2(img)
            #print(output.shape)
            probs = F.softmax(output,dim=1)
            max_prob,preds = torch.max(probs,dim=1)
            #print(len(labels))
            acc1 = (torch.sum(preds == labels).item() / len(preds))
            #compare = tmodel(img)
            #pdist = F.softmax(compare/4,dim=1)  ##Teacher distribution
            #max_prob1,preds1 = torch.max(pdist,dim=1)   #Teacher output
            #accu=[]
            a1.append(acc1)
            #gg = np.mean(accu)
            #soft_loss_teacher = nn.KLDivLoss(reduction='batchmean')(F.log_softmax(output / 4, dim=1), pdist)
            loss = nn.CrossEntropyLoss()(output,labels)
            #loss = 0.2*loss + 0.8*soft_loss_teacher
            l1.append(loss.item())
            loss.backward()
            optimizers2.step()
            optimizers2.zero_grad()
            
        for a,b in tqdm(val_loader, desc='Training', unit=' unit',leave=False,colour='red'):
                a = a.to(device)
                b = b.to(device)
                rr = smodel2(a)
                val_loss = nn.CrossEntropyLoss()(rr,b)
                l2.append(val_loss.item())
                probs = F.softmax(rr,dim=1)
                max_prob,preds = torch.max(probs,dim=1)
                acc = (torch.sum(preds == b).item() / len(preds))
                a2.append(acc)
        train_loss.append(np.mean(l1))
        val_los.append(np.mean(l2))
        train_acc.append(np.mean(a1))
        val_acc.append(np.mean(a2))
        end = timer()
        time = end - start
        time = time/60
        print(f'Epoch {i+1}/100: Train Loss==> {np.mean(l1):.2f} Val Loss==> {np.mean(l2):.2f} Train_Acc==> {np.mean(a1):.2f} Val_Acc==> {np.mean(a2):.2f}')   
        #print(i+1,loss,acc)

fig = plt.figure(figsize=(12, 5))
fig.add_subplot(1, 2, 1)
  
# showing image
plt.plot(train_loss,label='Training')
plt.plot(val_los,label='Testing')
plt.legend()
plt.title("Loss of Student(ShuffleNet Without KD) Model")
fig.add_subplot(1, 2, 2)
  
# showing image
plt.plot(train_acc,label='Training')
plt.plot(val_acc,label='Testing')
plt.legend()
plt.title("Accuracy of Studemt Model(ShuffleNet Without KD)")
plt.show()

classification_report(smodel2,test_loader)

train_loss1 = []
val_los1 = []
train_acc1 = []
val_acc1 = []
for i in range(80):
        l1 = []
        l2 = []
        a1 = []
        a2 = []
        start = timer()
        for img,labels in tqdm(sloader1, desc='Training', unit=' unit',leave=False,colour='red'):
            



           # print(end - start)
            
            
            
            img = img.to(device)
            labels = labels.to(device)
            
            output = smodel1(img)
            #print(output.shape)
            probs = F.softmax(output,dim=1)
            max_prob,preds = torch.max(probs,dim=1)
            #print(len(labels))
            acc1 = (torch.sum(preds == labels).item() / len(preds))
            compare = tmodel(img)
            pdist = F.softmax(compare/4,dim=1)  ##Teacher distribution
            max_prob1,preds1 = torch.max(pdist,dim=1)   #Teacher output
            #accu=[]
            a1.append(acc1)
            #gg = np.mean(accu)
            soft_loss_teacher = nn.KLDivLoss(reduction='batchmean')(F.log_softmax(output /4, dim=1), pdist)
            loss = nn.CrossEntropyLoss()(output,labels)
            loss = 0.2*16*loss + 0.8*16*soft_loss_teacher
            l1.append(loss.item())
            loss.backward()
            optimizers.step()
            optimizers.zero_grad()
            
        for a,b in tqdm(val_loader, desc='Training', unit=' unit',leave=False,colour='red'):
                a = a.to(device)
                b = b.to(device)
                rr = smodel1(a)
                val_loss = nn.CrossEntropyLoss()(rr,b)
                l2.append(val_loss.item())
                probs = F.softmax(rr,dim=1)
                max_prob,preds = torch.max(probs,dim=1)
                acc = (torch.sum(preds == b).item() / len(preds))
                a2.append(acc)
        train_loss1.append(np.mean(l1))
        val_los1.append(np.mean(l2))
        train_acc1.append(np.mean(a1))
        val_acc1.append(np.mean(a2))
        end = timer()
        time = end - start
        time = time/60
        print(f'Epoch {i+1}/80: Train Loss==> {np.mean(l1):.2f} Val Loss==> {np.mean(l2):.2f} Train_Acc==> {np.mean(a1):.2f} Val_Acc==> {np.mean(a2):.2f}')   
        #print(i+1,loss,acc)

fig = plt.figure(figsize=(12, 5))
fig.add_subplot(1, 2, 1)
  
# showing image
plt.plot(train_loss1,label='Training')
plt.plot(val_los1,label='Testing')
plt.legend()
plt.title("Loss of Student(ShuffleNet With KD) Model")
fig.add_subplot(1, 2, 2)
  
# showing image
plt.plot(train_acc1,label='Training')
plt.plot(val_acc1,label='Testing')
plt.legend()
plt.title("Accuracy of Student (ShuffleNet With KD)")
plt.show()

classification_report(smodel1,test_loader)

plt.plot(val_acc,label='Without KD')
plt.plot(val_acc1,label='With KD')
plt.legend()
plt.title("Accuracy of Student (ShuffleNet)")
plt.show()

import joblib
#joblib.dump(smodel4, 'csmodel_dnet.pkl')
joblib.dump(smodel1, 'shufflemodel_dnet.pkl')

"""# ResNET Teacher"""

smodel1 = torch.hub.load('pytorch/vision:v0.10.0', 'shufflenet_v2_x1_0', pretrained=1)
smodel1.fc = nn.Sequential(
    nn.Linear(in_features = 1024,out_features=22),
)

for name, child in smodel1.named_children():
   if name in ['stage3','stage4','conv5','fc']:
       print(name + ' is unfrozen')
       for param in child.parameters():
           param.requires_grad = True
   else:
       print(name + ' is frozen')
       for param in child.parameters():
           param.requires_grad = False

smodel2 = torch.hub.load('pytorch/vision:v0.10.0', 'shufflenet_v2_x1_0', pretrained=1)
smodel2.fc = nn.Sequential(
    nn.Linear(in_features = 1024,out_features=22),
)

for name, child in smodel2.named_children():
   if name in ['stage3','stage4','conv5','fc']:
       print(name + ' is unfrozen')
       for param in child.parameters():
           param.requires_grad = True
   else:
       print(name + ' is frozen')
       for param in child.parameters():
           param.requires_grad = False
optimizers = torch.optim.Adadelta(filter(lambda p: p.requires_grad, smodel1.parameters()), lr=0.02)
#smodel2 = smodel()
smodel1 = smodel1.cuda()
smodel2 = smodel2.cuda()
#optimizers = torch.optim.Adadelta(smodel1.parameters(), lr=0.6,weight_decay=1e-5)
optimizers2 = torch.optim.Adadelta(filter(lambda p: p.requires_grad, smodel2.parameters()), lr=0.02)

train_loss = []
val_los = []
train_acc = []
val_acc = []
for i in range(100):
        l1 = []
        l2 = []
        a1 = []
        a2 = []
        start = timer()
        for img,labels in tqdm(sloader1, desc='Training', unit=' unit',leave=False,colour='red'):
            



           # print(end - start)
            
            
            
            img = img.to(device)
            labels = labels.to(device)
            
            output = smodel2(img)
            #print(output.shape)
            probs = F.softmax(output,dim=1)
            max_prob,preds = torch.max(probs,dim=1)
            #print(len(labels))
            acc1 = (torch.sum(preds == labels).item() / len(preds))
            #compare = tmodel(img)
            #pdist = F.softmax(compare/4,dim=1)  ##Teacher distribution
            #max_prob1,preds1 = torch.max(pdist,dim=1)   #Teacher output
            #accu=[]
            a1.append(acc1)
            #gg = np.mean(accu)
            #soft_loss_teacher = nn.KLDivLoss(reduction='batchmean')(F.log_softmax(output / 4, dim=1), pdist)
            loss = nn.CrossEntropyLoss()(output,labels)
            #loss = 0.2*loss + 0.8*soft_loss_teacher
            l1.append(loss.item())
            loss.backward()
            optimizers2.step()
            optimizers2.zero_grad()
            
        for a,b in tqdm(val_loader, desc='Training', unit=' unit',leave=False,colour='red'):
                a = a.to(device)
                b = b.to(device)
                rr = smodel2(a)
                val_loss = nn.CrossEntropyLoss()(rr,b)
                l2.append(val_loss.item())
                probs = F.softmax(rr,dim=1)
                max_prob,preds = torch.max(probs,dim=1)
                acc = (torch.sum(preds == b).item() / len(preds))
                a2.append(acc)
        train_loss.append(np.mean(l1))
        val_los.append(np.mean(l2))
        train_acc.append(np.mean(a1))
        val_acc.append(np.mean(a2))
        end = timer()
        time = end - start
        time = time/60
        print(f'Epoch {i+1}/100: Train Loss==> {np.mean(l1):.2f} Val Loss==> {np.mean(l2):.2f} Train_Acc==> {np.mean(a1):.2f} Val_Acc==> {np.mean(a2):.2f}')   
        #print(i+1,loss,acc)

fig = plt.figure(figsize=(12, 5))
fig.add_subplot(1, 2, 1)
  
# showing image
plt.plot(train_loss,label='Training')
plt.plot(val_los,label='Testing')
plt.legend()
plt.title("Loss of Student(ShuffleNet Without KD) Model")
fig.add_subplot(1, 2, 2)
  
# showing image
plt.plot(train_acc,label='Training')
plt.plot(val_acc,label='Testing')
plt.legend()
plt.title("Accuracy of Studemt Model(ShuffleNet Without KD)")
plt.show()

classification_report(smodel2,test_loader)

train_loss1 = []
val_los1 = []
train_acc1 = []
val_acc1 = []
for i in range(80):
        l1 = []
        l2 = []
        a1 = []
        a2 = []
        start = timer()
        for img,labels in tqdm(sloader1, desc='Training', unit=' unit',leave=False,colour='red'):
            



           # print(end - start)
            
            
            
            img = img.to(device)
            labels = labels.to(device)
            
            output = smodel1(img)
            #print(output.shape)
            probs = F.softmax(output,dim=1)
            max_prob,preds = torch.max(probs,dim=1)
            #print(len(labels))
            acc1 = (torch.sum(preds == labels).item() / len(preds))
            compare = tmodel(img)
            pdist = F.softmax(compare/4,dim=1)  ##Teacher distribution
            max_prob1,preds1 = torch.max(pdist,dim=1)   #Teacher output
            #accu=[]
            a1.append(acc1)
            #gg = np.mean(accu)
            soft_loss_teacher = nn.KLDivLoss(reduction='batchmean')(F.log_softmax(output /4, dim=1), pdist)
            loss = nn.CrossEntropyLoss()(output,labels)
            loss = 0.2*16*loss + 0.8*16*soft_loss_teacher
            l1.append(loss.item())
            loss.backward()
            optimizers.step()
            optimizers.zero_grad()
            
        for a,b in tqdm(val_loader, desc='Training', unit=' unit',leave=False,colour='red'):
                a = a.to(device)
                b = b.to(device)
                rr = smodel1(a)
                val_loss = nn.CrossEntropyLoss()(rr,b)
                l2.append(val_loss.item())
                probs = F.softmax(rr,dim=1)
                max_prob,preds = torch.max(probs,dim=1)
                acc = (torch.sum(preds == b).item() / len(preds))
                a2.append(acc)
        train_loss1.append(np.mean(l1))
        val_los1.append(np.mean(l2))
        train_acc1.append(np.mean(a1))
        val_acc1.append(np.mean(a2))
        end = timer()
        time = end - start
        time = time/60
        print(f'Epoch {i+1}/80: Train Loss==> {np.mean(l1):.2f} Val Loss==> {np.mean(l2):.2f} Train_Acc==> {np.mean(a1):.2f} Val_Acc==> {np.mean(a2):.2f}')   
        #print(i+1,loss,acc)

fig = plt.figure(figsize=(12, 5))
fig.add_subplot(1, 2, 1)
  
# showing image
plt.plot(train_loss1,label='Training')
plt.plot(val_los1,label='Testing')
plt.legend()
plt.title("Loss of Student(ShuffleNet With KD ResNet101) Model")
fig.add_subplot(1, 2, 2)
  
# showing image
plt.plot(train_acc1,label='Training')
plt.plot(val_acc1,label='Testing')
plt.legend()
plt.title("Accuracy of Student (ShuffleNet With KD ResNet101)")
plt.show()

classification_report(smodel1,test_loader)

plt.plot(val_acc,label='Without KD')
plt.plot(val_acc1,label='With KD')
plt.legend()
plt.title("Accuracy of Student (ShuffleNet) with ResNET")
plt.show()

joblib.dump(smodel1, 'shufflemodel_rnet.pkl')

import torch
import torch.nn as nn

class smodel(nn.Module):
    def __init__(self):
        super(smodel, self).__init__()
        
        # Define convolutional layers
        self.conv1 = nn.Conv2d(in_channels=3, out_channels=8, kernel_size=3, padding=1)
        self.relu1 = nn.ReLU()
        self.pool1 = nn.MaxPool2d(kernel_size=5, stride=2)
        self.conv2 = nn.Conv2d(in_channels=8, out_channels=8, kernel_size=3, padding=1)
        self.relu2 = nn.ReLU()
        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)
        self.conv3 = nn.Conv2d(in_channels=8, out_channels=8, kernel_size=3, padding=1)
        self.relu3 = nn.ReLU()
        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)
        self.conv4 = nn.Conv2d(in_channels=8, out_channels=8, kernel_size=3, padding=1)
        self.relu4 = nn.ReLU()
        # Define fully connected layers
        self.fc1 = nn.Linear(in_features=392 , out_features=22)
        #self.relu3 = nn.ReLU()
        #self.fc2 = nn.Linear(in_features=250, out_features=22)
        
    def forward(self, x):
        x = self.conv1(x)
        x = self.relu1(x)
        x = self.pool1(x)
        x = self.conv2(x)
        x = self.relu2(x)
        x = self.pool2(x)
        x = self.conv3(x)
        x = self.relu3(x)
        x = self.pool3(x)
        x = self.conv4(x)
        x = self.relu4(x)
        #print(x.shape)
        x = torch.flatten(x,1)
        #print(x.shape)
        x = self.fc1(x)
        #x = self.relu3(x)
        #x = self.fc2(x)
        #print(x.shape)
        return x

smodel3 = smodel()
smodel3 = smodel3.cuda()
smodel4 = smodel()
smodel4 = smodel4.cuda()
optimizers = torch.optim.SGD(smodel4.parameters(), lr=0.03,momentum=0.9)
optimizers2 = torch.optim.Adadelta(smodel3.parameters(), lr=0.3)

train_loss = []
val_los = []
train_acc = []
val_acc = []
for i in range(100):
        l1 = []
        l2 = []
        a1 = []
        a2 = []
        start = timer()
        for img,labels in tqdm(sloader2, desc='Training', unit=' unit',leave=False,colour='red'):
            



           # print(end - start)
            
            
            
            img = img.to(device)
            labels = labels.to(device)
            
            output = smodel3(img)
            #print(output.shape)
            probs = F.softmax(output,dim=1)
            max_prob,preds = torch.max(probs,dim=1)
            #print(len(labels))
            acc1 = (torch.sum(preds == labels).item() / len(preds))
            #compare = smodel3(img)
            #pdist = F.softmax(compare/4,dim=1)  ##Teacher distribution
            #max_prob1,preds1 = torch.max(pdist,dim=1)   #Teacher output
            #accu=[]
            a1.append(acc1)
            #gg = np.mean(accu)
            #soft_loss_teacher = nn.KLDivLoss(reduction='batchmean')(F.log_softmax(output / 4, dim=1), pdist)
            loss = nn.CrossEntropyLoss()(output,labels)
            #loss = 0.2*loss + 0.8*soft_loss_teacher
            l1.append(loss.item())
            loss.backward()
            optimizers2.step()
            optimizers2.zero_grad()
            
        for a,b in tqdm(val_loader, desc='Training', unit=' unit',leave=False,colour='red'):
                a = a.to(device)
                b = b.to(device)
                rr = smodel3(a)
                val_loss = nn.CrossEntropyLoss()(rr,b)
                l2.append(val_loss.item())
                probs = F.softmax(rr,dim=1)
                max_prob,preds = torch.max(probs,dim=1)
                acc = (torch.sum(preds == b).item() / len(preds))
                a2.append(acc)
        train_loss.append(np.mean(l1))
        val_los.append(np.mean(l2))
        train_acc.append(np.mean(a1))
        val_acc.append(np.mean(a2))
        end = timer()
        time = end - start
        time = time/60
        print(f'Epoch {i+1}/100: Train Loss==> {np.mean(l1):.2f} Val Loss==> {np.mean(l2):.2f} Train_Acc==> {np.mean(a1):.2f} Val_Acc==> {np.mean(a2):.2f}')   
        #print(i+1,loss,acc)

fig = plt.figure(figsize=(12, 5))
fig.add_subplot(1, 2, 1)
  
# showing image
plt.plot(train_loss,label='Training')
plt.plot(val_los,label='Testing')
plt.legend()
plt.title("Loss of Student(Coustom model Without KD) Model")
fig.add_subplot(1, 2, 2)
  
# showing image
plt.plot(train_acc,label='Training')
plt.plot(val_acc,label='Testing')
plt.legend()
plt.title("Accuracy of Student Model(Coustom model Without KD)")
plt.show()

classification_report(smodel3,test_loader)

train_loss1 = []
val_los1 = []
train_acc1 = []
val_acc1 = []
for i in range(100):
        l1 = []
        l2 = []
        a1 = []
        a2 = []
        start = timer()
        for img,labels in tqdm(sloader2, desc='Training', unit=' unit',leave=False,colour='red'):
            



           # print(end - start)
            
            
            
            img = img.to(device)
            labels = labels.to(device)
            
            output = smodel4(img)
            #print(output.shape)
            probs = F.softmax(output,dim=1)
            max_prob,preds = torch.max(probs,dim=1)
            #print(len(labels))
            acc1 = (torch.sum(preds == labels).item() / len(preds))
            compare = tmodel2(img)
            pdist = F.softmax(compare/4,dim=1)  ##Teacher distribution
            max_prob1,preds1 = torch.max(pdist,dim=1)   #Teacher output
            #accu=[]
            a1.append(acc1)
            #gg = np.mean(accu)
            soft_loss_teacher = nn.KLDivLoss(reduction='batchmean')(F.log_softmax(output /4, dim=1), pdist)
            loss = nn.CrossEntropyLoss()(output,labels)
            loss = 0.2*1*loss + 0.8*1*soft_loss_teacher
            l1.append(loss.item())
            loss.backward()
            optimizers.step()
            optimizers.zero_grad()
            
        for a,b in tqdm(val_loader, desc='Training', unit=' unit',leave=False,colour='red'):
                a = a.to(device)
                b = b.to(device)
                rr = smodel4(a)
                val_loss = nn.CrossEntropyLoss()(rr,b)
                l2.append(val_loss.item())
                probs = F.softmax(rr,dim=1)
                max_prob,preds = torch.max(probs,dim=1)
                acc = (torch.sum(preds == b).item() / len(preds))
                a2.append(acc)
        train_loss1.append(np.mean(l1))
        val_los1.append(np.mean(l2))
        train_acc1.append(np.mean(a1))
        val_acc1.append(np.mean(a2))
        end = timer()
        time = end - start
        time = time/60
        print(f'Epoch {i+1}/100: Train Loss==> {np.mean(l1):.2f} Val Loss==> {np.mean(l2):.2f} Train_Acc==> {np.mean(a1):.2f} Val_Acc==> {np.mean(a2):.2f}')   
        #print(i+1,loss,acc)

fig = plt.figure(figsize=(12, 5))
fig.add_subplot(1, 2, 1)
  
# showing image
plt.plot(train_loss1,label='Training')
plt.plot(val_los1,label='Testing')
plt.legend()
plt.title("Loss of Student(Custom With KD) Model")
fig.add_subplot(1, 2, 2)
  
# showing image
plt.plot(train_acc1,label='Training')
plt.plot(val_acc1,label='Testing')
plt.legend()
plt.title("Accuracy of Student (Custom With KD)")
plt.show()

plt.plot(val_acc,label='Without KD')
plt.plot(val_acc1,label='With KD')
plt.legend()
plt.title("Accuracy of Student (Custom Model)")
plt.show()

joblib.dump(smodel4, 'csmodel_rnet.pkl')

"""# Ensemble"""

import joblib
smodel1 = joblib.load('shufflemodel_dnet.pkl').cuda()
ssmodel2 = joblib.load('csmodel_dnet.pkl').cuda()
smodel3 = joblib.load('shufflemodel_rnet.pkl').cuda()
ssmodel4 = joblib.load('csmodel_rnet.pkl').cuda()

start = timer()
predictions1 = []
predictions2 = []
predictions3 = []
predictions4 = []
max1 = []
max2 = []
max3 = []
max4 = []
accu = []
l_act = []
i = 0
for images_test,labels_test in test_loader:
    #i = i+1```````````````````````````````
    images_test = images_test.to(device)
    labels_test = labels_test.to(device)
    ypred = ssmodel2(images_test)
    probs = F.softmax(ypred,dim=1)
    max_prob,preds = torch.max(probs,dim=1)
    max1.append(max_prob)
    predictions2.append(preds)
    
    
    ypred = smodel1(images_test)
    probs = F.softmax(ypred,dim=1)
    max_prob,preds = torch.max(probs,dim=1)
    max2.append(max_prob)
    predictions1.append(preds)
    
    
    ypred = smodel3(images_test)
    probs = F.softmax(ypred,dim=1)
    max_prob,preds = torch.max(probs,dim=1)
    max3.append(max_prob)
    predictions3.append(preds)
    
    ypred = ssmodel4(images_test)
    probs = F.softmax(ypred,dim=1)
    max_prob,preds = torch.max(probs,dim=1)
    max4.append(max_prob)
    predictions4.append(preds)
    #print(preds)
    #print(max_prob)
    #acc = (torch.sum(preds == labels_test).item() / len(preds))
    #accu.append(acc)
    #predictions.append(preds)
    l_act.append(labels_test)
    print(labels_test)

p1 = []
for i in predictions1:
    for j in i:
        p1.append(j.item())
p2 = []
for i in predictions2:
    for j in i:
        p2.append(j.item())
p3 = []
for i in predictions3:
    for j in i:
        p3.append(j.item())
p4 = []
for i in predictions4:
    for j in i:
        p4.append(j.item())

ytrue = []
for i in l_act:
    for j in i:
        ytrue.append(j.item())

m1 = []
m2 = []
m3 = []
m4 = []
for i in max1:
    for j in i:
        m1.append(j.item())
for i in max2:
    for j in i:
        m2.append(j.item())
for i in max3:
    for j in i:
        m3.append(j.item())
for i in max4:
    for j in i:
        m4.append(j.item())
    
#predictions[0]

from collections import Counter
def most_frequent(List):
    return max(set(List), key = List.count)

def ensemble(p1,p2,p3,p4):
    final_pred = []
    for i in range(len(p1)):
        compare = []
        prob = []
        compare.append(p1[i])
        compare.append(p2[i])
        compare.append(p3[i])
        compare.append(p4[i])
        prob.append(m1[i])
        prob.append(m2[i])
        prob.append(m3[i])
        prob.append(m4[i])
       # print(prob)
        a = most_frequent(compare) 
        d = Counter(compare)
        count = d[a]
        if(count==1):
            maximum = np.argmax(prob,0)
            final_pred.append(compare[maximum])
        else:
            final_pred.append(a)
    return final_pred

final = ensemble(p1,p2,p3,p4)

report = pd.DataFrame()
report['Actual'] = ytrue
report['Predicted'] = final
report_ensemble = report.copy()
from sklearn.metrics import classification_report
print(classification_report(report['Actual'], report['Predicted']))